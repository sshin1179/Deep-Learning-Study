{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Is All You Need\n",
    "\n",
    "At its core, the self-attention mechanism revolves around the interplay of three components: **key**, **query**, and **value**. These are vital for understanding how information is weighted and propagated in attention models, such as the Transformer.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_{k}}}\\right) \\cdot V $$\n",
    "\n",
    "When $Q = K$, the term $QK^{T}$ captures the self-attention, indicating how similar elements within the matrix $Q$ are to one another.\n",
    "\n",
    "## Why Use $\\sqrt{d_k}$?\n",
    "\n",
    "[proof.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5df62dc5-72a1-4bf6-ac20-b804fe52d000/proof.pdf)\n",
    "\n",
    "### Lemma 1\n",
    "\n",
    "Under the assumption that the components of $q$ and $k$ are independent random variables with mean 0 and variance 1, their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_{i}k_{i}$ has mean 0 and variance $d_{k}$.\n",
    "\n",
    "The mean can be determined using the **linearity of expectation**:\n",
    "\n",
    "$$ E[q \\cdot k] = E\\left[\\sum_{i=1}^{d_k} q_i k_i\\right] $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{d_k} E[q_ik_i] $$\n",
    "\n",
    "Given the assumption that random variables are i.i.d (independently identically distributed):\n",
    "\n",
    "$$ = \\sum_{i=1}^{d_k} E[q_i]E[k_i] = 0 $$\n",
    "\n",
    "Thus, the mean of $q \\cdot k$ equals 0.\n",
    "\n",
    "For variance, although variance is not strictly linear in the way that expectation is, in this context, since the random variables are independent, the variance of their sum is the sum of their variances. Hence, using a principle similar to the **linearity of expectation**:\n",
    "\n",
    "$$ \\text{var}[q \\cdot k] = \\text{var}\\left[\\sum_{i=1}^{d_k}q_ik_i\\right] $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{d_k}\\text{var}[q_ik_i] = d_k $$\n",
    "\n",
    "To make the dot product have a mean of 0 and standard deviation of 1, it's divided by $\\sqrt{d_k}$. However, nowadays, this normalization is often omitted since a normal distribution is not always assumed, especially when layer normalization is not used.\n",
    "\n",
    "### Scaled Dot Product Attention\n",
    "\n",
    "**Scaled Dot Product Attention** refers to the process of this calculation.\n",
    "\n",
    "Given that **Query**, **Key**, and **Value** are all $3 \\times 1$ matrices:\n",
    "\n",
    "$$ \n",
    "Q = K = V = \\begin{bmatrix} \n",
    "v_1 \\\\ \n",
    "v_2 \\\\ \n",
    "v_3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Since $QK^{T}$ results in a $3 \\times 3$ matrix:\n",
    "\n",
    "$$ \n",
    "QK^T = \\begin{bmatrix} \n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & v_1 \\cdot v_3 \\\\ \n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & v_2 \\cdot v_3 \\\\ \n",
    "v_3 \\cdot v_1 & v_3 \\cdot v_2 & v_3 \\cdot v_3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We then divide $QK^{T}$ by $\\sqrt{d_k}$, obtaining the **attention weight**:\n",
    "\n",
    "$$ \n",
    "\\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) = \\begin{bmatrix} \n",
    "w_{11} & w_{12} & w_{13} \\\\ \n",
    "w_{21} & w_{22} & w_{23} \\\\ \n",
    "w_{31} & w_{32} & w_{33} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Given the value matrix, we compute:\n",
    "\n",
    "$$ \n",
    "\\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\times V = \\begin{bmatrix} \n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "y_3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The attention mechanism gauges the similarity between a *query* (the word we're focusing on) and a *key* (the word we're comparing against). The resulting similarity scores are then used to weigh the importance of words in the **Value** matrix.\n",
    "\n",
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # matrix multiplication\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "\n",
    "        # mask fill\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmax function (+ dropout optional)\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "\n",
    "        # matrix multiplication between KQ^T * V\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        # output and attn weight returned\n",
    "        return output, attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
