{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Is All You Need\n",
    "## Scaled Dot Product Attention\n",
    "\n",
    "At its core, the self-attention mechanism revolves around the interplay of three components: **key**, **query**, and **value**. These are vital for understanding how information is weighted and propagated in attention models, such as the Transformer.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_{k}}}\\right) \\cdot V $$\n",
    "\n",
    "When $Q = K$, the term $QK^{T}$ captures the self-attention, indicating how similar elements within the matrix $Q$ are to one another.\n",
    "\n",
    "### Why Use $\\sqrt{d_k}$?\n",
    "Under the assumption that the components of $q$ and $k$ are independent random variables with mean 0 and variance 1 (it is quite theoretical assumption that is not realistic for most cases), their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_{i}k_{i}$ has mean 0 and variance $d_{k}$.\n",
    "\n",
    "The mean can be determined using the **linearity of expectation**:\n",
    "\n",
    "$$ E[q \\cdot k] = E\\left[\\sum_{i=1}^{d_k} q_i k_i\\right] $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{d_k} E[q_ik_i] $$\n",
    "\n",
    "Given the assumption that random variables are i.i.d (independently identically distributed):\n",
    "\n",
    "$$ = \\sum_{i=1}^{d_k} E[q_i]E[k_i] = 0 $$\n",
    "\n",
    "Thus, the mean of $q \\cdot k$ equals 0.\n",
    "\n",
    "For variance, although variance is not strictly linear in the way that expectation is, in this context, since the random variables are independent, the variance of their sum is the sum of their variances. Hence, using a principle similar to the **linearity of expectation**:\n",
    "\n",
    "$$ \\text{var}[q \\cdot k] = \\text{var}\\left[\\sum_{i=1}^{d_k}q_ik_i\\right] $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{d_k}\\text{var}[q_ik_i] = d_k $$\n",
    "\n",
    "To make the dot product have a mean of 0 and standard deviation of 1, it's divided by $\\sqrt{d_k}$. However, nowadays, this normalization is often omitted since a normal distribution is not always assumed, especially when layer normalization is not used. **Scaled Dot Product Attention** refers to the process of this calculation. Given that **Query**, **Key**, and **Value** are all $3 \\times 1$ matrices:\n",
    "\n",
    "$$ \n",
    "Q = K = V = \\begin{bmatrix} \n",
    "v_1 \\\\ \n",
    "v_2 \\\\ \n",
    "v_3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Since $QK^{T}$ results in a $3 \\times 3$ matrix:\n",
    "\n",
    "$$ \n",
    "QK^T = \\begin{bmatrix} \n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & v_1 \\cdot v_3 \\\\ \n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & v_2 \\cdot v_3 \\\\ \n",
    "v_3 \\cdot v_1 & v_3 \\cdot v_2 & v_3 \\cdot v_3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We then divide $QK^{T}$ by $\\sqrt{d_k}$, obtaining the **attention weight**:\n",
    "\n",
    "$$ \n",
    "\\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) = \\begin{bmatrix} \n",
    "w_{11} & w_{12} & w_{13} \\\\ \n",
    "w_{21} & w_{22} & w_{23} \\\\ \n",
    "w_{31} & w_{32} & w_{33} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Given the value matrix, we compute:\n",
    "\n",
    "$$ \n",
    "\\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\times V = \\begin{bmatrix} \n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "y_3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The attention mechanism gauges the similarity between a *query* (the word we're focusing on) and a *key* (the word we're comparing against). The resulting similarity scores are then used to weigh the importance of words in the **Value** matrix. See below example code to understand how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install torch torchtext\n",
    "# !/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -float('inf'))\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadAttention & PositionWiseFeedForward\n",
    "\n",
    "Now, it is the moment to introduce **MultiHeadAttention** -- where multiple **ScaledDotProductAttention** modules are applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, d_k * n_head, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, d_k * n_head, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, d_v * n_head, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        self.attention = ScaledDotProductAttention(temperature=torch.sqrt(torch.tensor(d_k).float()))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q = q.size(0), q.size(1)\n",
    "\n",
    "        residual = q  # Store the residual connection\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_q, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_q, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_v)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x + residual)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, d_ff, n_head, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_ff, dropout=dropout)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        enc_output, attn = self.self_attn(src, src, src, mask=mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, attn\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, d_ff, n_head, n_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, d_k, d_v, d_ff, n_head, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        attns = []\n",
    "        for layer in self.layers:\n",
    "            src, attn = layer(src, mask=mask)\n",
    "            attns.append(attn)\n",
    "        return src, attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Code to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output Shape: torch.Size([1, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have all the previous classes defined as provided\n",
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# 1. Tokenization and Vocabulary\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for sentence in data_iter:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "sentences = [\"Hello World\"]\n",
    "vocab = build_vocab_from_iterator(yield_tokens(sentences), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "tokenized_sentence = [vocab[token] for token in tokenizer(\"Hello World\")]\n",
    "sentence_tensor = torch.tensor(tokenized_sentence, dtype=torch.long).unsqueeze(0)  # (1, len(sentence))\n",
    "\n",
    "# 2. Embedding\n",
    "embedding_dim = 512\n",
    "embedding = torch.nn.Embedding(len(vocab), embedding_dim)\n",
    "embedded_sentence = embedding(sentence_tensor)\n",
    "\n",
    "# 3. Encoding with TransformerEncoder\n",
    "# Assuming d_model=512, d_k=d_v=64, d_ff=2048, n_head=8, n_layers=6\n",
    "encoder = TransformerEncoder(d_model=512, d_k=64, d_v=64, d_ff=2048, n_head=8, n_layers=6)\n",
    "enc_output, attns = encoder(embedded_sentence)\n",
    "\n",
    "print(\"Encoder Output Shape:\", enc_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 512]), torch.Size([1, 2, 512]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape, enc_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./Attention Is All You Need.ipynb to pdf\n",
      "[NbConvertApp] Writing 42392 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/bin/jupyter-nbconvert\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/jupyter_core/application.py\", line 285, in launch_instance\n",
      "    return super().launch_instance(argv=argv, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.11/site-packages/traitlets/config/application.py\", line 1046, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/nbconvertapp.py\", line 410, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/nbconvertapp.py\", line 585, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/nbconvertapp.py\", line 551, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/nbconvertapp.py\", line 477, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/exporters/templateexporter.py\", line 389, in from_filename\n",
      "    return super().from_filename(filename, resources, **kw)  # type:ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/exporters/exporter.py\", line 201, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/exporters/templateexporter.py\", line 395, in from_file\n",
      "    return super().from_file(file_stream, resources, **kw)  # type:ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/exporters/exporter.py\", line 220, in from_file\n",
      "    return self.from_notebook_node(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/exporters/pdf.py\", line 199, in from_notebook_node\n",
      "    self.run_latex(tex_file)\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/exporters/pdf.py\", line 168, in run_latex\n",
      "    return self.run_command(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/nbconvert/exporters/pdf.py\", line 122, in run_command\n",
      "    raise OSError(msg)\n",
      "OSError: xelatex not found on PATH, if you have not installed xelatex you may need to do so. Find further instructions at https://nbconvert.readthedocs.io/en/latest/install.html#installing-tex.\n"
     ]
    }
   ],
   "source": [
    "from base_lib import convert_ipynb_to_pdf\n",
    "\n",
    "convert_ipynb_to_pdf('./Attention Is All You Need.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
