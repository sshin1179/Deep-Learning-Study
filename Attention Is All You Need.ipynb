{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Is All You Need\n",
    "## Scaled Dot Product Attention\n",
    "\n",
    "At its core, the self-attention mechanism revolves around the interplay of three components: **key**, **query**, and **value**. These are vital for understanding how information is weighted and propagated in attention models, such as the Transformer.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_{k}}}\\right) \\cdot V $$\n",
    "\n",
    "When $Q = K$, the term $QK^{T}$ captures the self-attention, indicating how similar elements within the matrix $Q$ are to one another.\n",
    "\n",
    "### Why Use $\\sqrt{d_k}$?\n",
    "Under the assumption that the components of $q$ and $k$ are independent random variables with mean 0 and variance 1 (it is quite theoretical assumption that is not realistic for most cases), their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_{i}k_{i}$ has mean 0 and variance $d_{k}$.\n",
    "\n",
    "The mean can be determined using the **linearity of expectation**:\n",
    "\n",
    "$$ E[q \\cdot k] = E\\left[\\sum_{i=1}^{d_k} q_i k_i\\right] $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{d_k} E[q_ik_i] $$\n",
    "\n",
    "Given the assumption that random variables are i.i.d (independently identically distributed):\n",
    "\n",
    "$$ = \\sum_{i=1}^{d_k} E[q_i]E[k_i] = 0 $$\n",
    "\n",
    "Thus, the mean of $q \\cdot k$ equals 0.\n",
    "\n",
    "For variance, although variance is not strictly linear in the way that expectation is, in this context, since the random variables are independent, the variance of their sum is the sum of their variances. Hence, using a principle similar to the **linearity of expectation**:\n",
    "\n",
    "$$ \\text{var}[q \\cdot k] = \\text{var}\\left[\\sum_{i=1}^{d_k}q_ik_i\\right] $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{d_k}\\text{var}[q_ik_i] = d_k $$\n",
    "\n",
    "To make the dot product have a mean of 0 and standard deviation of 1, it's divided by $\\sqrt{d_k}$. However, nowadays, this normalization is often omitted since a normal distribution is not always assumed, especially when layer normalization is not used. **Scaled Dot Product Attention** refers to the process of this calculation. Given that **Query**, **Key**, and **Value** are all $3 \\times 1$ matrices:\n",
    "\n",
    "$$ \n",
    "Q = K = V = \\begin{bmatrix} \n",
    "v_1 \\\\ \n",
    "v_2 \\\\ \n",
    "v_3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Since $QK^{T}$ results in a $3 \\times 3$ matrix:\n",
    "\n",
    "$$ \n",
    "QK^T = \\begin{bmatrix} \n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & v_1 \\cdot v_3 \\\\ \n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & v_2 \\cdot v_3 \\\\ \n",
    "v_3 \\cdot v_1 & v_3 \\cdot v_2 & v_3 \\cdot v_3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We then divide $QK^{T}$ by $\\sqrt{d_k}$, obtaining the **attention weight**:\n",
    "\n",
    "$$ \n",
    "\\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) = \\begin{bmatrix} \n",
    "w_{11} & w_{12} & w_{13} \\\\ \n",
    "w_{21} & w_{22} & w_{23} \\\\ \n",
    "w_{31} & w_{32} & w_{33} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Given the value matrix, we compute:\n",
    "\n",
    "$$ \n",
    "\\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\times V = \\begin{bmatrix} \n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "y_3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The attention mechanism gauges the similarity between a *query* (the word we're focusing on) and a *key* (the word we're comparing against). The resulting similarity scores are then used to weigh the importance of words in the **Value** matrix. See below example code to understand how it goes:\n",
    "\n",
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[[ 8.7608e-01,  3.3927e-01, -1.3244e+00,  ..., -4.0746e-02,\n",
      "          -1.6846e-01, -1.5164e-01],\n",
      "         [ 1.4763e+00,  1.6987e+00,  9.0214e-01,  ..., -1.2537e+00,\n",
      "           8.5446e-01,  1.3122e+00],\n",
      "         [ 1.3544e+00,  9.5870e-01, -4.4449e-02,  ...,  2.3614e+00,\n",
      "           7.4944e-01,  1.7983e+00],\n",
      "         ...,\n",
      "         [ 3.3140e-01,  2.2513e+00, -1.4790e+00,  ..., -3.9521e-01,\n",
      "          -1.2430e+00,  3.5047e-01],\n",
      "         [ 3.9767e-01,  2.7016e+00, -1.7748e+00,  ..., -4.7426e-01,\n",
      "          -1.4916e+00,  4.2055e-01],\n",
      "         [ 3.3140e-01,  2.2513e+00, -1.4790e+00,  ..., -3.9521e-01,\n",
      "          -1.2430e+00,  3.5047e-01]],\n",
      "\n",
      "        [[ 1.2716e+00,  2.2604e+00, -3.6325e-01,  ..., -2.4550e-03,\n",
      "          -1.1870e-01, -1.8570e+00],\n",
      "         [ 4.0824e-01,  9.8044e-01,  1.0579e+00,  ..., -6.8351e-02,\n",
      "           2.1602e+00, -3.0626e-01],\n",
      "         [ 5.2817e-03,  2.1607e-02, -1.1288e-02,  ..., -3.0273e-03,\n",
      "          -7.8106e-03, -1.4656e-03],\n",
      "         ...,\n",
      "         [ 3.9771e-01,  2.7016e+00, -1.7747e+00,  ..., -4.7433e-01,\n",
      "          -1.4915e+00,  4.2032e-01],\n",
      "         [ 3.9771e-01,  2.7016e+00, -1.7747e+00,  ..., -4.7433e-01,\n",
      "          -1.4915e+00,  4.2032e-01],\n",
      "         [ 3.9771e-01,  2.7016e+00, -1.7747e+00,  ..., -4.7433e-01,\n",
      "          -1.4915e+00,  4.2032e-01]],\n",
      "\n",
      "        [[ 3.9766e-01,  2.7017e+00, -1.7748e+00,  ..., -4.7434e-01,\n",
      "          -1.4916e+00,  4.2056e-01],\n",
      "         [ 1.2560e+00,  1.3017e+00, -2.7146e-01,  ...,  4.7079e-01,\n",
      "          -5.7247e-01,  1.2709e+00],\n",
      "         [ 8.7295e-01, -8.0647e-01, -1.2203e+00,  ..., -1.1202e+00,\n",
      "           1.2367e+00,  2.8106e+00],\n",
      "         ...,\n",
      "         [ 3.9765e-01,  2.7017e+00, -1.7748e+00,  ..., -4.7433e-01,\n",
      "          -1.4916e+00,  4.2054e-01],\n",
      "         [ 3.9765e-01,  2.7016e+00, -1.7748e+00,  ..., -4.7434e-01,\n",
      "          -1.4916e+00,  4.2055e-01],\n",
      "         [ 3.3139e-01,  2.2514e+00, -1.4790e+00,  ..., -3.9529e-01,\n",
      "          -1.2430e+00,  3.5048e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Attention Weights: tensor([[[1.1047e+00, 2.1812e-04, 1.2233e-03, 8.7574e-04, 6.8907e-04,\n",
      "          6.8907e-04, 0.0000e+00, 6.8907e-04, 6.8907e-04, 6.8907e-04],\n",
      "         [2.2619e-04, 1.1088e+00, 1.2803e-04, 7.2777e-04, 2.0966e-04,\n",
      "          2.0966e-04, 2.0966e-04, 2.0966e-04, 0.0000e+00, 2.0966e-04],\n",
      "         [4.5647e-05, 4.6070e-06, 1.1108e+00, 2.5666e-05, 3.3737e-05,\n",
      "          3.3737e-05, 3.3737e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [2.8330e-04, 2.2704e-04, 2.2251e-04, 1.1096e+00, 1.2389e-04,\n",
      "          0.0000e+00, 1.2389e-04, 1.2389e-04, 1.2389e-04, 1.2389e-04],\n",
      "         [1.9576e-05, 5.7437e-06, 2.5684e-05, 1.0880e-05, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01],\n",
      "         [1.9576e-05, 5.7437e-06, 2.5684e-05, 1.0880e-05, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 0.0000e+00, 1.8517e-01, 1.8517e-01],\n",
      "         [1.9576e-05, 5.7437e-06, 2.5684e-05, 1.0880e-05, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01],\n",
      "         [1.9576e-05, 5.7437e-06, 2.5684e-05, 1.0880e-05, 1.8517e-01,\n",
      "          0.0000e+00, 1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01],\n",
      "         [1.9576e-05, 5.7437e-06, 2.5684e-05, 1.0880e-05, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01],\n",
      "         [1.9576e-05, 5.7437e-06, 2.5684e-05, 1.0880e-05, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01, 0.0000e+00]],\n",
      "\n",
      "        [[1.1085e+00, 2.2738e-04, 2.1794e-04, 9.8116e-05, 3.5037e-04,\n",
      "          3.5037e-04, 3.5037e-04, 3.5037e-04, 3.5037e-04, 3.5037e-04],\n",
      "         [2.3580e-03, 1.1028e+00, 9.3823e-04, 1.3127e-03, 0.0000e+00,\n",
      "          6.1041e-04, 6.1041e-04, 6.1041e-04, 6.1041e-04, 6.1041e-04],\n",
      "         [1.9845e-03, 8.2384e-04, 0.0000e+00, 4.0243e-04, 1.4052e-03,\n",
      "          0.0000e+00, 1.4052e-03, 1.4052e-03, 1.4052e-03, 1.4052e-03],\n",
      "         [8.3428e-04, 1.0763e-03, 3.7579e-04, 1.1060e+00, 4.6301e-04,\n",
      "          4.6301e-04, 4.6301e-04, 4.6301e-04, 4.6301e-04, 4.6301e-04],\n",
      "         [6.3321e-05, 1.0638e-05, 2.7889e-05, 9.8408e-06, 1.8517e-01,\n",
      "          0.0000e+00, 1.8517e-01, 0.0000e+00, 1.8517e-01, 1.8517e-01],\n",
      "         [6.3321e-05, 1.0638e-05, 2.7889e-05, 9.8408e-06, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01],\n",
      "         [0.0000e+00, 1.0638e-05, 0.0000e+00, 9.8408e-06, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01],\n",
      "         [6.3321e-05, 1.0638e-05, 2.7889e-05, 9.8408e-06, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01],\n",
      "         [6.3321e-05, 1.0638e-05, 2.7889e-05, 9.8408e-06, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01],\n",
      "         [6.3321e-05, 1.0638e-05, 2.7889e-05, 9.8408e-06, 1.8517e-01,\n",
      "          1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01, 1.8517e-01]],\n",
      "\n",
      "        [[1.8518e-01, 1.1987e-05, 9.4514e-06, 5.7438e-06, 4.6722e-06,\n",
      "          1.8518e-01, 1.8518e-01, 1.8518e-01, 1.8518e-01, 1.8518e-01],\n",
      "         [4.5960e-03, 1.0698e+00, 6.0307e-03, 5.5311e-03, 2.2027e-03,\n",
      "          0.0000e+00, 0.0000e+00, 4.5960e-03, 4.5960e-03, 4.5960e-03],\n",
      "         [2.6019e-04, 4.3300e-04, 1.1000e+00, 3.3173e-03, 5.7934e-03,\n",
      "          2.6019e-04, 2.6019e-04, 2.6019e-04, 2.6019e-04, 2.6019e-04],\n",
      "         [2.0849e-04, 0.0000e+00, 4.3740e-03, 1.1026e+00, 2.3594e-03,\n",
      "          2.0849e-04, 2.0849e-04, 2.0849e-04, 2.0849e-04, 2.0849e-04],\n",
      "         [1.5482e-04, 1.9036e-04, 6.9733e-03, 2.1539e-03, 1.1009e+00,\n",
      "          1.5482e-04, 1.5482e-04, 1.5482e-04, 1.5482e-04, 1.5482e-04],\n",
      "         [1.8518e-01, 1.1987e-05, 9.4514e-06, 5.7438e-06, 4.6722e-06,\n",
      "          1.8518e-01, 1.8518e-01, 1.8518e-01, 1.8518e-01, 1.8518e-01],\n",
      "         [1.8518e-01, 1.1987e-05, 9.4514e-06, 5.7438e-06, 4.6722e-06,\n",
      "          0.0000e+00, 1.8518e-01, 1.8518e-01, 0.0000e+00, 1.8518e-01],\n",
      "         [1.8518e-01, 1.1987e-05, 0.0000e+00, 5.7438e-06, 4.6722e-06,\n",
      "          1.8518e-01, 1.8518e-01, 1.8518e-01, 1.8518e-01, 1.8518e-01],\n",
      "         [1.8518e-01, 1.1987e-05, 9.4514e-06, 0.0000e+00, 4.6722e-06,\n",
      "          1.8518e-01, 1.8518e-01, 1.8518e-01, 1.8518e-01, 1.8518e-01],\n",
      "         [1.8518e-01, 1.1987e-05, 9.4514e-06, 5.7438e-06, 4.6722e-06,\n",
      "          1.8518e-01, 0.0000e+00, 1.8518e-01, 1.8518e-01, 1.8518e-01]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([3, 10, 64]) torch.Size([3, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Attention Module\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -torch.inf)\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"This is a sample.\", \"Attention mechanisms are powerful.\", \"Scaled dot product is interesting.\"]\n",
    "\n",
    "# Simple tokenization (split by space) and creating a vocabulary\n",
    "tokens = [sentence.split() for sentence in sentences]\n",
    "vocab = set(word for sentence in tokens for word in sentence)\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Convert tokens to integers\n",
    "token_ids = [[word_to_idx[word] for word in token] for token in tokens]\n",
    "\n",
    "# Pad sequences (assuming a max sequence length of 10 for simplicity)\n",
    "max_len, d_model = 10, 64\n",
    "padded_token_ids = [token_id + [0] * (max_len - len(token_id)) for token_id in token_ids]\n",
    "\n",
    "# Convert to tensor\n",
    "input_tensor = torch.tensor(padded_token_ids)\n",
    "\n",
    "# Embedding (assuming d_model = 64)\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "embedded_input = embedding(input_tensor)\n",
    "\n",
    "# Attention mechanism\n",
    "temperature = torch.sqrt(torch.tensor(d_model).float())\n",
    "# temperature = torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "sdp_attention = ScaledDotProductAttention(temperature)\n",
    "\n",
    "# Using the same embedded input for q, k, and v for simplicity\n",
    "output, attn_weights = sdp_attention(embedded_input, embedded_input, embedded_input)\n",
    "\n",
    "# Display the results\n",
    "print(\"Output:\", output)\n",
    "print(\"Attention Weights:\", attn_weights)\n",
    "print(output.shape, attn_weights.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
